{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3_1_simple_RNN_after.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kagglematsuo/Rapid_Challenge_Report/blob/master/3_1_simple_RNN_after.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03cddebb-8205-451c-828e-24f9c92b85ad"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/DNN_code_colab_lesson_3_4')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzGmsHRwO-bi"
      },
      "source": [
        "# simple RNN after\n",
        "### バイナリ加算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KNSG0aKXO-bk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1cca7c24-e25e-4115-990e-5a46dea66aab"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "# Xavier\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# He\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "#         z[:,t+1] = functions.relu(u[:,t+1])\n",
        "#         z[:,t+1] = np.tanh(u[:,t+1])    \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.0275827834748734\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[1 1 1 0 0 0 1 1]\n",
            "117 + 110 = 127\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.9063824699845191\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "56 + 108 = 0\n",
            "------------\n",
            "iters:200\n",
            "Loss:1.0960928386270647\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "101 + 36 = 255\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.9995122416981268\n",
            "Pred:[1 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "70 + 51 = 244\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.9799578126331715\n",
            "Pred:[0 0 0 1 0 1 1 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "109 + 47 = 22\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.0163843575937623\n",
            "Pred:[1 1 1 1 1 1 0 1]\n",
            "True:[0 1 0 1 0 0 1 1]\n",
            "58 + 25 = 253\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.358574664308965\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "56 + 69 = 0\n",
            "------------\n",
            "iters:700\n",
            "Loss:1.005199614842825\n",
            "Pred:[0 0 0 0 0 1 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "82 + 23 = 5\n",
            "------------\n",
            "iters:800\n",
            "Loss:1.0332156485349584\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 1]\n",
            "93 + 84 = 0\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.9578608322876772\n",
            "Pred:[1 0 1 1 0 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "72 + 38 = 182\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.0442148293285192\n",
            "Pred:[0 0 0 1 0 0 1 1]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "79 + 95 = 19\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.9958992205875846\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 0 1 1 1 1 1 1]\n",
            "60 + 3 = 1\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.9563502882265048\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 0 1 1 1 0 1]\n",
            "91 + 2 = 119\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.9506607820070158\n",
            "Pred:[1 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "38 + 71 = 239\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.9745131196567964\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 0 1 0 1 1 1 1]\n",
            "4 + 43 = 119\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.9712820433044219\n",
            "Pred:[1 1 0 0 0 1 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "2 + 70 = 196\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.7515224951392236\n",
            "Pred:[0 0 1 1 1 1 1 1]\n",
            "True:[0 0 1 0 1 1 1 1]\n",
            "9 + 38 = 63\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.5378501196973802\n",
            "Pred:[0 0 0 0 0 0 1 1]\n",
            "True:[0 0 0 0 1 0 1 0]\n",
            "5 + 5 = 3\n",
            "------------\n",
            "iters:1800\n",
            "Loss:1.0255986309079628\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "74 + 117 = 129\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.6088808735319581\n",
            "Pred:[0 0 1 0 0 1 0 0]\n",
            "True:[0 0 1 0 0 1 0 0]\n",
            "2 + 34 = 36\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.8816773959162871\n",
            "Pred:[1 1 0 1 1 0 0 0]\n",
            "True:[1 1 0 1 0 0 1 0]\n",
            "110 + 100 = 216\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.7898382414536923\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "42 + 83 = 117\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.8425800852969558\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "94 + 26 = 116\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.7940238806790695\n",
            "Pred:[0 0 0 0 0 1 1 0]\n",
            "True:[0 0 0 0 1 0 0 0]\n",
            "1 + 7 = 6\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.8588547847524985\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 1 1 1 0 1 1 1]\n",
            "127 + 120 = 135\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.6172478079858714\n",
            "Pred:[0 1 1 0 0 1 1 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "88 + 30 = 102\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.4786575728817527\n",
            "Pred:[1 0 1 1 0 1 1 1]\n",
            "True:[1 0 1 1 0 1 1 1]\n",
            "104 + 79 = 183\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.7888315192198894\n",
            "Pred:[1 1 0 0 0 0 1 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "97 + 48 = 195\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.9414443567659261\n",
            "Pred:[0 1 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "81 + 62 = 79\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.7076513491690876\n",
            "Pred:[0 0 0 1 1 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "5 + 93 = 26\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.49778734669650393\n",
            "Pred:[1 1 1 1 0 0 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "104 + 9 = 241\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.692515651290956\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "85 + 41 = 254\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.5315998240592327\n",
            "Pred:[1 1 1 0 1 1 1 0]\n",
            "True:[1 1 1 0 1 1 1 0]\n",
            "126 + 112 = 238\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.3631595373421588\n",
            "Pred:[1 0 1 1 0 1 0 1]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "82 + 99 = 181\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.4504275808675303\n",
            "Pred:[0 0 1 1 0 1 1 1]\n",
            "True:[0 0 1 1 0 0 1 1]\n",
            "2 + 49 = 55\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.8537230941423692\n",
            "Pred:[1 0 1 0 0 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "110 + 89 = 167\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.7879462647776666\n",
            "Pred:[1 0 1 0 1 1 0 1]\n",
            "True:[1 0 1 1 0 0 0 1]\n",
            "91 + 86 = 173\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.08559484856170367\n",
            "Pred:[0 0 0 0 0 1 0 0]\n",
            "True:[0 0 0 0 0 1 0 0]\n",
            "2 + 2 = 4\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.8058259732098656\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "15 + 63 = 126\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.14490491027240343\n",
            "Pred:[0 0 0 0 1 1 1 1]\n",
            "True:[0 0 0 0 1 1 1 1]\n",
            "15 + 0 = 15\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.5216931316624139\n",
            "Pred:[0 1 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "43 + 29 = 64\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.41831512484420297\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 1 1 0 0 1 0 1]\n",
            "120 + 109 = 133\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.3483804120671722\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "49 + 86 = 135\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.11688479226489694\n",
            "Pred:[0 0 1 1 0 1 0 1]\n",
            "True:[0 0 1 1 0 1 0 1]\n",
            "52 + 1 = 53\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.33562974649308347\n",
            "Pred:[1 1 1 0 0 0 1 1]\n",
            "True:[1 1 1 0 0 0 1 1]\n",
            "100 + 127 = 227\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.4466605249119334\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "124 + 76 = 136\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.2312288224177022\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "46 + 63 = 109\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.5659807524471878\n",
            "Pred:[1 1 1 1 0 0 1 1]\n",
            "True:[1 1 1 0 1 1 1 1]\n",
            "118 + 121 = 243\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.25212513200611125\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "78 + 42 = 120\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.09191181369363614\n",
            "Pred:[1 0 1 1 1 0 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "117 + 67 = 184\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.5557663721239655\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 1 0 0 0 1 0 0]\n",
            "69 + 127 = 164\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.04344379278995854\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "78 + 27 = 105\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.10619450270335172\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "111 + 50 = 161\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.0304530045511906\n",
            "Pred:[1 0 1 1 1 1 1 0]\n",
            "True:[1 0 1 1 1 1 1 0]\n",
            "65 + 125 = 190\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.3447206191739104\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "127 + 17 = 144\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.015989019402828916\n",
            "Pred:[0 1 0 1 1 0 1 0]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "89 + 1 = 90\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.07996781302741802\n",
            "Pred:[1 0 0 1 0 0 0 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "92 + 53 = 145\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.10523384469258555\n",
            "Pred:[0 1 1 0 0 1 1 0]\n",
            "True:[0 1 1 0 0 1 1 0]\n",
            "60 + 42 = 102\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.150020412721074\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "51 + 78 = 129\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.10791845766009556\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "60 + 59 = 119\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.027898256886867284\n",
            "Pred:[1 0 0 1 0 0 1 1]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "79 + 68 = 147\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.019492758506589002\n",
            "Pred:[1 0 1 0 0 1 1 1]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "48 + 119 = 167\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.04579308194936412\n",
            "Pred:[0 0 1 1 0 1 0 1]\n",
            "True:[0 0 1 1 0 1 0 1]\n",
            "19 + 34 = 53\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.010196016313840919\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "2 + 99 = 101\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.0339258874164214\n",
            "Pred:[1 0 0 1 1 0 1 1]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "110 + 45 = 155\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.03266867328930534\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "126 + 4 = 130\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.018586836150949887\n",
            "Pred:[0 1 1 0 1 0 1 1]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "78 + 29 = 107\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.010777800990802779\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "51 + 97 = 148\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.011172604085751867\n",
            "Pred:[1 0 1 1 1 0 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "67 + 117 = 184\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.010674105111351365\n",
            "Pred:[0 0 0 0 1 1 1 1]\n",
            "True:[0 0 0 0 1 1 1 1]\n",
            "4 + 11 = 15\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.0036805217304261397\n",
            "Pred:[0 0 1 1 0 0 0 1]\n",
            "True:[0 0 1 1 0 0 0 1]\n",
            "36 + 13 = 49\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.005786822359212029\n",
            "Pred:[1 0 0 1 0 0 1 1]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "40 + 107 = 147\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.011533940944372423\n",
            "Pred:[1 0 1 1 0 0 0 1]\n",
            "True:[1 0 1 1 0 0 0 1]\n",
            "63 + 114 = 177\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.006804117213225573\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "87 + 40 = 127\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.007203886963349251\n",
            "Pred:[0 1 0 0 0 0 1 0]\n",
            "True:[0 1 0 0 0 0 1 0]\n",
            "29 + 37 = 66\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.007895820522423692\n",
            "Pred:[0 1 0 0 0 1 1 1]\n",
            "True:[0 1 0 0 0 1 1 1]\n",
            "53 + 18 = 71\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.01046714477472345\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "46 + 8 = 54\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.012102078372516607\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "114 + 25 = 139\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.01012926079534131\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "32 + 94 = 126\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.0036513630246144896\n",
            "Pred:[1 0 1 0 0 1 1 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "93 + 73 = 166\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.009583109011365285\n",
            "Pred:[1 0 0 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "24 + 108 = 132\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.013397584254978187\n",
            "Pred:[0 1 1 0 1 0 1 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "92 + 14 = 106\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.004240935327515205\n",
            "Pred:[1 1 0 0 1 1 1 0]\n",
            "True:[1 1 0 0 1 1 1 0]\n",
            "113 + 93 = 206\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.006862147615430365\n",
            "Pred:[1 0 1 0 0 1 0 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "86 + 79 = 165\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.008717935388876817\n",
            "Pred:[1 0 0 1 0 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "28 + 122 = 150\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.005107338394590462\n",
            "Pred:[0 0 1 1 0 1 0 0]\n",
            "True:[0 0 1 1 0 1 0 0]\n",
            "36 + 16 = 52\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.0018616466443961291\n",
            "Pred:[1 0 1 1 0 1 0 1]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "68 + 113 = 181\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.005612994555858222\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "26 + 50 = 76\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.003687647016936128\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "87 + 37 = 124\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.001500546978806767\n",
            "Pred:[0 0 0 1 1 0 0 0]\n",
            "True:[0 0 0 1 1 0 0 0]\n",
            "17 + 7 = 24\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.007718374461585536\n",
            "Pred:[0 1 1 1 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "70 + 42 = 112\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.0009139248075875424\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "50 + 71 = 121\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.00432320983883024\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "2 + 80 = 82\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.005352484130214054\n",
            "Pred:[1 0 0 1 1 0 1 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "93 + 61 = 154\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.001446410450360339\n",
            "Pred:[1 0 1 0 1 1 0 1]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "86 + 87 = 173\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.0016549569129343763\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "10 + 119 = 129\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.0017287238880113681\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "9 + 96 = 105\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0026409106974135026\n",
            "Pred:[1 0 0 1 1 0 1 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "119 + 35 = 154\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.005071276874947519\n",
            "Pred:[1 1 0 0 0 1 1 0]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "124 + 74 = 198\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.002105034450313358\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "45 + 115 = 160\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHxPVNwY16A2"
      },
      "source": [
        "ReLU関数を活性化関数に使った場合"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w-H2voRJ1xf0",
        "outputId": "96e38eb7-e306-456a-e231-19311a3cbac0"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "# Xavier\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# He\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        # z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "        z[:,t+1] = functions.relu(u[:,t+1])\n",
        "#         z[:,t+1] = np.tanh(u[:,t+1])    \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        # delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.7804512245657305\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 0 0 1 0]\n",
            "91 + 87 = 255\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "56 + 91 = 0\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/DNN_code_colab_lesson_3_4/common/functions.py:6: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1 + np.exp(-x))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iters:200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 1 1 1 0]\n",
            "95 + 127 = 0\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "53 + 32 = 0\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 0 1 1 1 0]\n",
            "13 + 1 = 0\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "41 + 40 = 0\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "27 + 42 = 0\n",
            "------------\n",
            "iters:700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "106 + 4 = 0\n",
            "------------\n",
            "iters:800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 1 1 0 0]\n",
            "118 + 86 = 0\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "92 + 53 = 0\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "106 + 1 = 0\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "108 + 29 = 0\n",
            "------------\n",
            "iters:1200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "108 + 39 = 0\n",
            "------------\n",
            "iters:1300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "51 + 98 = 0\n",
            "------------\n",
            "iters:1400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 1 0 0]\n",
            "21 + 47 = 0\n",
            "------------\n",
            "iters:1500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "4 + 126 = 0\n",
            "------------\n",
            "iters:1600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "127 + 0 = 0\n",
            "------------\n",
            "iters:1700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "31 + 74 = 0\n",
            "------------\n",
            "iters:1800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "127 + 34 = 0\n",
            "------------\n",
            "iters:1900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "17 + 110 = 0\n",
            "------------\n",
            "iters:2000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "27 + 124 = 0\n",
            "------------\n",
            "iters:2100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "13 + 64 = 0\n",
            "------------\n",
            "iters:2200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "1 + 109 = 0\n",
            "------------\n",
            "iters:2300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "82 + 22 = 0\n",
            "------------\n",
            "iters:2400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 1 0]\n",
            "66 + 80 = 0\n",
            "------------\n",
            "iters:2500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "26 + 79 = 0\n",
            "------------\n",
            "iters:2600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "34 + 83 = 0\n",
            "------------\n",
            "iters:2700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "105 + 24 = 0\n",
            "------------\n",
            "iters:2800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "90 + 60 = 0\n",
            "------------\n",
            "iters:2900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 0 1 0]\n",
            "59 + 127 = 0\n",
            "------------\n",
            "iters:3000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 0 0 1 0 1]\n",
            "14 + 23 = 0\n",
            "------------\n",
            "iters:3100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 1 0]\n",
            "96 + 98 = 0\n",
            "------------\n",
            "iters:3200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "27 + 50 = 0\n",
            "------------\n",
            "iters:3300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 1 1]\n",
            "126 + 69 = 0\n",
            "------------\n",
            "iters:3400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "66 + 62 = 0\n",
            "------------\n",
            "iters:3500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "64 + 87 = 0\n",
            "------------\n",
            "iters:3600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "119 + 9 = 0\n",
            "------------\n",
            "iters:3700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "43 + 95 = 0\n",
            "------------\n",
            "iters:3800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 1 1 1]\n",
            "90 + 93 = 0\n",
            "------------\n",
            "iters:3900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 1 1]\n",
            "28 + 23 = 0\n",
            "------------\n",
            "iters:4000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "8 + 110 = 0\n",
            "------------\n",
            "iters:4100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "123 + 51 = 0\n",
            "------------\n",
            "iters:4200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "1 + 81 = 0\n",
            "------------\n",
            "iters:4300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 0 1 0 0]\n",
            "120 + 92 = 0\n",
            "------------\n",
            "iters:4400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "18 + 124 = 0\n",
            "------------\n",
            "iters:4500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "74 + 98 = 0\n",
            "------------\n",
            "iters:4600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 1 0 0]\n",
            "20 + 32 = 0\n",
            "------------\n",
            "iters:4700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "8 + 78 = 0\n",
            "------------\n",
            "iters:4800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "49 + 111 = 0\n",
            "------------\n",
            "iters:4900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "67 + 76 = 0\n",
            "------------\n",
            "iters:5000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 1 0 1]\n",
            "94 + 95 = 0\n",
            "------------\n",
            "iters:5100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "89 + 62 = 0\n",
            "------------\n",
            "iters:5200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "39 + 48 = 0\n",
            "------------\n",
            "iters:5300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "87 + 65 = 0\n",
            "------------\n",
            "iters:5400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 1 0]\n",
            "43 + 119 = 0\n",
            "------------\n",
            "iters:5500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 1 1 0]\n",
            "7 + 95 = 0\n",
            "------------\n",
            "iters:5600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 0 1]\n",
            "90 + 103 = 0\n",
            "------------\n",
            "iters:5700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 1 1]\n",
            "86 + 109 = 0\n",
            "------------\n",
            "iters:5800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "78 + 94 = 0\n",
            "------------\n",
            "iters:5900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 0 1 1]\n",
            "34 + 25 = 0\n",
            "------------\n",
            "iters:6000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "7 + 107 = 0\n",
            "------------\n",
            "iters:6100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 1 1]\n",
            "28 + 23 = 0\n",
            "------------\n",
            "iters:6200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "104 + 8 = 0\n",
            "------------\n",
            "iters:6300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 1 1 0 1 1]\n",
            "21 + 6 = 0\n",
            "------------\n",
            "iters:6400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "44 + 33 = 0\n",
            "------------\n",
            "iters:6500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "30 + 24 = 0\n",
            "------------\n",
            "iters:6600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "42 + 98 = 0\n",
            "------------\n",
            "iters:6700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 0 0 1]\n",
            "1 + 56 = 0\n",
            "------------\n",
            "iters:6800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 1 1 0]\n",
            "117 + 113 = 0\n",
            "------------\n",
            "iters:6900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "98 + 32 = 0\n",
            "------------\n",
            "iters:7000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "46 + 84 = 0\n",
            "------------\n",
            "iters:7100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "78 + 35 = 0\n",
            "------------\n",
            "iters:7200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "97 + 77 = 0\n",
            "------------\n",
            "iters:7300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "50 + 97 = 0\n",
            "------------\n",
            "iters:7400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 1 0 1 1 0]\n",
            "13 + 9 = 0\n",
            "------------\n",
            "iters:7500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 1 0 0 0 0]\n",
            "117 + 123 = 0\n",
            "------------\n",
            "iters:7600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "73 + 62 = 0\n",
            "------------\n",
            "iters:7700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "29 + 53 = 0\n",
            "------------\n",
            "iters:7800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "68 + 11 = 0\n",
            "------------\n",
            "iters:7900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "46 + 68 = 0\n",
            "------------\n",
            "iters:8000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "87 + 33 = 0\n",
            "------------\n",
            "iters:8100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "27 + 95 = 0\n",
            "------------\n",
            "iters:8200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "98 + 52 = 0\n",
            "------------\n",
            "iters:8300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "90 + 55 = 0\n",
            "------------\n",
            "iters:8400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "25 + 91 = 0\n",
            "------------\n",
            "iters:8500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 0 0]\n",
            "41 + 7 = 0\n",
            "------------\n",
            "iters:8600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 1 0 1]\n",
            "46 + 7 = 0\n",
            "------------\n",
            "iters:8700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 0 0 1 0 1]\n",
            "2 + 3 = 0\n",
            "------------\n",
            "iters:8800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "100 + 23 = 0\n",
            "------------\n",
            "iters:8900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 0 1 1 0 1]\n",
            "15 + 30 = 0\n",
            "------------\n",
            "iters:9000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 1 1]\n",
            "17 + 34 = 0\n",
            "------------\n",
            "iters:9100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 1 0 0 1 1]\n",
            "3 + 16 = 0\n",
            "------------\n",
            "iters:9200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "32 + 90 = 0\n",
            "------------\n",
            "iters:9300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "71 + 61 = 0\n",
            "------------\n",
            "iters:9400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 0 1 1]\n",
            "25 + 34 = 0\n",
            "------------\n",
            "iters:9500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "95 + 24 = 0\n",
            "------------\n",
            "iters:9600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 1 1 0 1 0]\n",
            "17 + 9 = 0\n",
            "------------\n",
            "iters:9700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 0 1 1]\n",
            "18 + 57 = 0\n",
            "------------\n",
            "iters:9800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "111 + 18 = 0\n",
            "------------\n",
            "iters:9900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "93 + 74 = 0\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATgklEQVR4nO3df7Dld13f8efrnLOJ/NCQuHcwEHCTEaU4mgjXFlpat7WFJaUiUxizdQqizM5Yp2o7bSHj1EyHvyi2YzWVdYdut3bsaispYCrFFq3bqZV6U0Oy/AgEqbgg3UsyhZI6LZt994/zvZubved7zsne7+7lc/b5mDmTc7/f736/n+/9bl77vp/v+55vqgpJ0moZ7fUAJEnDM9wlaQUZ7pK0ggx3SVpBhrskraDJXh14//79deDAgb06vCQ16b777vtCVa0t2m7Pwv3AgQNsbGzs1eElqUlJ/mCZ7RZOyyQ5nuRsktM9669L8qtJPpzkI0ne9FQHK0ka1jJz7ieAQ3PW/wjw0aq6FTgI/KMk1+x+aJKkS7Uw3KvqFPDovE2Ar00S4JndtueGGZ4k6VIM0S1zN/AngM8BDwI/VlXnZ22Y5EiSjSQbm5ubAxxakjTLEOH+SuB+4DnAbcDdSb5u1oZVdayq1qtqfW1t4c1eSdIlGiLc3wTcU1MPA58GXjjAfiVJl2iIcP8M8N0ASZ4NfAvw+wPsV5J0iRb2uSc5ybQLZn+SM8BdwD6AqjoKvA04keRBIMBbquoLl2vAD33+f3PvA5/jjX/6APufee3lOowkNW1huFfV4QXrPwe8YrARLfDw2S/zs7/xMK/+9ucY7pLUo7nPlhmPAsC58zMbciRJNBjuky7cHz/vE6QkqU9z4T4eb1Xuhrsk9Wku3Lcq9/OGuyT1ai7cn5hzN9wlqU9z4T4ZTYfsnLsk9Wsu3K3cJWmx5sL9iW4ZWyElqU9z4X6hcn/cyl2S+jQX7pOxfe6StEh74e6cuyQt1Fy4j+2WkaSFmgt3K3dJWqy5cB/bLSNJCzUX7lbukrRYc+E+9lMhJWmhheGe5HiSs0lO96z/u0nu716nkzye5Ibhhzq19fED9rlLUr9lKvcTwKG+lVX1jqq6rapuA+4EfquqHh1ofDuM7XOXpIUWhntVnQKWDevDwMldjWgB59wlabHB5tyTPJ1phf/uofY5i90ykrTYkDdU/wrwX+ZNySQ5kmQjycbm5uYlHWQcK3dJWmTIcL+DBVMyVXWsqtaran1tbe2SDjIahVGcc5ekeQYJ9yTXAd8FvHeI/S0yGY2s3CVpjsmiDZKcBA4C+5OcAe4C9gFU1dFus9cCv15Vj12mcT7JeBQrd0maY2G4V9XhJbY5wbRl8oqYjGKfuyTN0dxvqMK0191uGUnq12S4T0Zxzl2S5mgy3J1zl6T5mgx3u2Ukab4mw93KXZLmazLcnXOXpPmaDPdp5W63jCT1aTbc7XOXpH5Nhvtk7Jy7JM3TZLiP7ZaRpLmaDPeJ3TKSNFeT4T4ehXPeUJWkXk2Gu5W7JM3XZLiP7XOXpLmaDHcrd0mar8lwH49G9rlL0hxNhruVuyTNtzDckxxPcjbJ6TnbHExyf5KPJPmtYYe403hst4wkzbNM5X4CONS3MsmzgJ8DvqeqvhV4/TBD62flLknzLQz3qjoFPDpnk78G3FNVn+m2PzvQ2HrZLSNJ8w0x5/7NwPVJ/lOS+5K8oW/DJEeSbCTZ2NzcvOQDWrlL0nxDhPsEeAnwl4FXAn8/yTfP2rCqjlXVelWtr62tXfIB/WwZSZpvMsA+zgCPVNVjwGNJTgG3Ap8YYN8zWblL0nxDVO7vBV6eZJLk6cCfAj42wH57TT/P3W4ZSeqzsHJPchI4COxPcga4C9gHUFVHq+pjSf498ABwHnhXVfW2TQ7Byl2S5lsY7lV1eIlt3gG8Y5ARLWHa5264S1KfJn9DdRwrd0map8lwn3R97lUGvCTN0mS4j0fTYVu8S9JsTYb7ZBwAP19Gkno0Ge7j0TTcnXeXpNmaDPeJ4S5JczUZ7lbukjRfk+G+Vbnb6y5JszUZ7lvdMlbukjRbk+Fu5S5J8zUZ7hfm3H1ItiTN1GS42+cuSfM1Ge52y0jSfE2Gu3PukjRfk+Fut4wkzddkuFu5S9J8C8M9yfEkZ5PMfLpSkoNJvpjk/u71k8MP88memHP3hqokzbLMA7JPAHcDvzBnm/9cVa8eZERLuFC52wopSTMtrNyr6hTw6BUYy9LslpGk+Yaac39Zkg8neX+Sb+3bKMmRJBtJNjY3Ny/5YE/0uRvukjTLEOH+34FvrKpbgZ8F3tO3YVUdq6r1qlpfW1u75APaLSNJ8+063KvqS1X15e79rwH7kuzf9cjmsFtGkubbdbgn+YYk6d7/yW6fj+x2v/PYLSNJ8y3slklyEjgI7E9yBrgL2AdQVUeB1wE/nOQc8MfAHVV1WUtqK3dJmm9huFfV4QXr72baKnnF2C0jSfM1+huq02Hb5y5JszUZ7uOxlbskzdNkuDvnLknzNRnudstI0nxNhruVuyTN12S42y0jSfM1Ge4XumUMd0maqclwt3KXpPmaDHc/z12S5msy3EejkNgtI0l9mgx3mFbvzrlL0mzNhvt4FOfcJalHs+E+GY2s3CWpR7PhbuUuSf2aDffpnLs3VCVplmbD3cpdkvotDPckx5OcTXJ6wXbfmeRcktcNN7x+k1Hsc5ekHstU7ieAQ/M2SDIG3g78+gBjWsp4bOUuSX0WhntVnQIeXbDZ3wTeDZwdYlDLsFtGkvrtes49yXOB1wLv3P1wluecuyT1G+KG6k8Db6mqha0rSY4k2Uiysbm5uauD2i0jSf0mA+xjHfilJAD7gduTnKuq91y8YVUdA44BrK+v76rstnKXpH67DvequnnrfZITwL2zgn1ofraMJPVbGO5JTgIHgf1JzgB3AfsAquroZR3dHFbuktRvYbhX1eFld1ZVP7Cr0TwFk9HIPndJ6uFvqErSCmo23Cdju2UkqU+z4W7lLkn92g332C0jSX3aDXcrd0nq1Wy4T+fcDXdJmqXZcB+PRlbuktSj2XD3s2UkqV+z4T4eBbNdkmZrNtyt3CWpX7PhbreMJPVrNtz9VEhJ6tdsuI9HIx73g8MkaaZmw90+d0nq12y4O+cuSf2aDXe7ZSSpX7PhPh6F8wXnrd4laYeF4Z7keJKzSU73rH9NkgeS3J9kI8nLhx/mTpNRAHi8DHdJutgylfsJ4NCc9R8Ebq2q24AfBN41wLgWGo+mQ3feXZJ2WhjuVXUKeHTO+i9XXSifnwFckbTdqtztmJGknQaZc0/y2iQfB/4d0+q9b7sj3dTNxubm5q6OOd6alrHXXZJ2GCTcq+rfVtULge8F3jZnu2NVtV5V62tra7s65mS8VbnbMSNJFxu0W6abwrklyf4h9zvLhcrdaRlJ2mHX4Z7km5Kke/9i4Frgkd3udxHn3CWp32TRBklOAgeB/UnOAHcB+wCq6ijwV4E3JPkK8MfA9227wXrZ2C0jSf0WhntVHV6w/u3A2wcb0ZKs3CWpX9O/oQrwuDdUJWmHZsPdyl2S+jUb7luV+zn73CVph2bDfavP3RuqkrRTs+G+1S3jtIwk7dRsuE/8JSZJ6tVsuF+Yc7dbRpJ2aDbcrdwlqV+z4T62FVKSejUb7pOtjx+wFVKSdmg23K3cJalfs+Fun7sk9Ws23O2WkaR+zYa73TKS1K/ZcHfOXZL6NRvuEx/WIUm9FoZ7kuNJziY53bP++5M8kOTBJL+d5Nbhh7mTlbsk9Vumcj8BHJqz/tPAd1XVtwFvA44NMK6FLsy5P+4NVUm62DKP2TuV5MCc9b+97cvfAW7a/bAWG4+t3CWpz9Bz7j8EvL9vZZIjSTaSbGxubu7qQHbLSFK/wcI9yZ9nGu5v6dumqo5V1XpVra+tre3qeM65S1K/hdMyy0jy7cC7gFdV1SND7HMRu2Ukqd+uK/ckzwfuAf56VX1i90NaTle4W7lL0gwLK/ckJ4GDwP4kZ4C7gH0AVXUU+Eng64GfSwJwrqrWL9eAt42LySg87scPSNIOy3TLHF6w/s3Amwcb0VMwHsXKXZJmaPY3VGHaMePnuUvSTk2Hu5W7JM3WdLhPxiO7ZSRphqbD3cpdkmZrOtztlpGk2ZoOdyt3SZqt+XB3zl2Sdmo+3K3cJWmnpsPdPndJmq3pcB+PRjxehrskXazpcJ845y5JMzUd7s65S9JsTYe7fe6SNFvT4T4ehXPeUJWkHZoO98nYOXdJmqXpcB+PRs65S9IMC8M9yfEkZ5Oc7ln/wiT/Ncn/TfJ3hh9iP7tlJGm2ZSr3E8ChOesfBX4U+KkhBvRU2C0jSbMtDPeqOsU0wPvWn62q3wW+MuTAlmG3jCTN1vicu5W7JM1yRcM9yZEkG0k2Njc3d70/59wlabYrGu5Vdayq1qtqfW1tbdf7G49G9rlL0gxNT8tYuUvSbJNFGyQ5CRwE9ic5A9wF7AOoqqNJvgHYAL4OOJ/kx4EXVdWXLtuoO+Oxc+6SNMvCcK+qwwvWfx64abARPQV2y0jSbE1Py9gtI0mzNR3uzrlL0mxNh7ufLSNJszUd7lbukjRb0+E+7sK9fI6qJD1J0+E+GQXA6l2SLtJ0uI/H03B33l2SnqzpcLdyl6TZmg738Wg6fCt3SXqypsPdyl2SZms63MejrTl3P4JAkrZrOtyt3CVptqbD/ULl7me6S9KTNB3uk7GVuyTN0nS42y0jSbM1He7OuUvSbE2Hu90ykjTbwnBPcjzJ2SSne9Ynyc8keTjJA0lePPwwZ7Nyl6TZlqncTwCH5qx/FfCC7nUEeOfuh7WcJyp3w12StlvmGaqnkhyYs8lrgF+o6efu/k6SZyW5sar+aKAx9pp0N1R/9OTv8bR948t9OEkaxPd95/N485+95bIeY2G4L+G5wB9u+/pMt2xHuCc5wrS65/nPf/6uD/xtN13H619yE4/9v3O73pckXSn7n3ntZT/GEOG+tKo6BhwDWF9f3/VcynVP28c7Xn/rrsclSatmiG6ZzwLP2/b1Td0ySdIeGSLc3we8oeuaeSnwxSsx3y5J6rdwWibJSeAgsD/JGeAuYB9AVR0Ffg24HXgY+D/Amy7XYCVJy1mmW+bwgvUF/MhgI5Ik7VrTv6EqSZrNcJekFWS4S9IKMtwlaQVlej90Dw6cbAJ/cIl/fD/whQGH04qr8byvxnOGq/O8r8Zzhqd+3t9YVWuLNtqzcN+NJBtVtb7X47jSrsbzvhrPGa7O874azxku33k7LSNJK8hwl6QV1Gq4H9vrAeyRq/G8r8ZzhqvzvK/Gc4bLdN5NzrlLkuZrtXKXJM1huEvSCmou3JMcSvJQ90Dut+71eHYjyfOS/GaSjyb5SJIf65bfkOQ/JPlk99/ru+W9DyNP8sZu+08meeNendOykoyT/F6Se7uvb07yoe7cfjnJNd3ya7uvH+7WH9i2jzu75Q8leeXenMnyukdQ/kqSjyf5WJKXrfq1TvK3ur/bp5OcTPI1q3itkxxPcjbJ6W3LBru2SV6S5MHuz/xMkiwcVFU18wLGwKeAW4BrgA8DL9rrce3ifG4EXty9/1rgE8CLgH8IvLVb/lbg7d3724H3AwFeCnyoW34D8Pvdf6/v3l+/1+e34Nz/NvCvgHu7r/81cEf3/ijww937vwEc7d7fAfxy9/5F3fW/Fri5+3sx3uvzWnDO/wJ4c/f+GuBZq3ytmT5u89PA07Zd4x9YxWsN/DngxcDpbcsGu7bAf+u2TfdnX7VwTHv9TXmK38CXAR/Y9vWdwJ17Pa4Bz++9wF8CHgJu7JbdCDzUvf954PC27R/q1h8Gfn7b8idt99X2Yvq0rg8CfwG4t/sL+wVgcvF1Bj4AvKx7P+m2y8XXfvt2X40v4Lou6HLR8pW91jzxfOUbumt3L/DKVb3WwIGLwn2Qa9ut+/i25U/aru/V2rRM38O4m9f9CPodwIeAZ9cTT7P6PPDs7n3f+bf2fflp4O8B57uvvx74X1W19aTz7eO/cG7d+i9227d2zjcDm8A/76aj3pXkGazwta6qzwI/BXwG+COm1+4+Vv9abxnq2j63e3/x8rlaC/eVlOSZwLuBH6+qL21fV9N/qlemXzXJq4GzVXXfXo/lCpsw/bH9nVX1HcBjTH9Uv2AFr/X1wGuY/sP2HOAZwKE9HdQe2Ytr21q4r9zDuJPsYxrsv1hV93SL/2eSG7v1NwJnu+V959/S9+XPAN+T5H8Av8R0auafAM9KsvVksO3jv3Bu3frrgEdo65xhWm2dqaoPdV//CtOwX+Vr/ReBT1fVZlV9BbiH6fVf9Wu9Zahr+9nu/cXL52ot3H8XeEF3t/0apjdd3rfHY7pk3R3vfwZ8rKr+8bZV7wO27pS/kelc/NbyWQ8j/wDwiiTXd9XSK7plX3Wq6s6quqmqDjC9fr9RVd8P/Cbwum6zi89563vxum776pbf0XVY3Ay8gOlNp69KVfV54A+TfEu36LuBj7LC15rpdMxLkzy9+7u+dc4rfa23GeTaduu+lOSl3ffxDdv21W+vb0Jcwk2L25l2lXwK+Im9Hs8uz+XlTH9UewC4v3vdznSe8YPAJ4H/CNzQbR/gn3bn/iCwvm1fP8j0IeUPA2/a63Nb8vwP8kS3zC1M/4d9GPg3wLXd8q/pvn64W3/Ltj//E9334iGW6B7Y6xdwG7DRXe/3MO2IWOlrDfwD4OPAaeBfMu14WblrDZxkel/hK0x/SvuhIa8tsN59Dz8F3M1FN+Znvfz4AUlaQa1Ny0iSlmC4S9IKMtwlaQUZ7pK0ggx3SVpBhrskrSDDXZJW0P8HOZpyKAhNq4EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1Fclzc62o-u"
      },
      "source": [
        "活性化関数にtanhを使った場合"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c9e4rBrg10Vz",
        "outputId": "d75b1543-72b1-496e-b3a5-c7bcc97e8305"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "# Xavier\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# He\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        # z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "#         z[:,t+1] = functions.relu(u[:,t+1])\n",
        "        z[:,t+1] = np.tanh(u[:,t+1])    \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        # delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.6563780128164\n",
            "Pred:[1 1 0 0 1 0 0 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "49 + 116 = 201\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.2193748535163387\n",
            "Pred:[1 0 1 1 1 1 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "14 + 91 = 189\n",
            "------------\n",
            "iters:200\n",
            "Loss:1.2105806415638078\n",
            "Pred:[0 0 0 0 0 0 1 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "27 + 98 = 3\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.803468369894282\n",
            "Pred:[0 1 0 0 1 1 0 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "72 + 15 = 77\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.8596206247751155\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "32 + 44 = 116\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.9318353021041835\n",
            "Pred:[0 0 0 0 1 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "27 + 72 = 11\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.6733930098763455\n",
            "Pred:[0 1 0 0 0 1 0 1]\n",
            "True:[0 1 0 1 1 1 0 1]\n",
            "21 + 72 = 69\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.8063359936626546\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 0 0 1 1]\n",
            "113 + 66 = 255\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.8543688480315658\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[1 1 0 0 1 1 1 0]\n",
            "112 + 94 = 126\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.0997905917099837\n",
            "Pred:[0 1 1 1 0 0 1 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "123 + 21 = 114\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.8771017087003764\n",
            "Pred:[1 0 0 0 1 0 0 1]\n",
            "True:[0 1 0 1 1 0 0 1]\n",
            "35 + 54 = 137\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.0578269844680412\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "59 + 39 = 254\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.6693655710967845\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 0 0 0 1 1 0 1]\n",
            "9 + 4 = 109\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.6974895675359466\n",
            "Pred:[1 1 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "76 + 98 = 238\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.7239756883533978\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 1 0 1 0 0]\n",
            "3 + 17 = 0\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.6102095746283976\n",
            "Pred:[0 1 0 1 1 1 1 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "35 + 57 = 94\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.5665005474901645\n",
            "Pred:[1 1 0 0 0 0 1 1]\n",
            "True:[0 1 0 0 0 0 1 1]\n",
            "66 + 1 = 195\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.6977994748094107\n",
            "Pred:[1 0 1 1 1 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "78 + 82 = 184\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.5256552707875324\n",
            "Pred:[0 0 1 0 1 0 1 1]\n",
            "True:[0 0 1 0 0 0 1 1]\n",
            "13 + 22 = 43\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.9553309301141566\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "7 + 70 = 127\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.9891376941828718\n",
            "Pred:[1 1 0 1 0 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "127 + 10 = 209\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.7176232981537813\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "103 + 24 = 255\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.9370469707154755\n",
            "Pred:[0 0 1 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 1 0]\n",
            "31 + 19 = 32\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.9104143486457269\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 1 0 0 0 1 0 1]\n",
            "109 + 88 = 129\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.9242266590300489\n",
            "Pred:[1 1 0 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "36 + 80 = 212\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.5060963499752827\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 0 1 1 0]\n",
            "98 + 4 = 110\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.753509934334382\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "31 + 83 = 108\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.9181020464018114\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 0 1 1 1 0 1 0]\n",
            "61 + 125 = 130\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.2815143622655381\n",
            "Pred:[0 0 1 0 1 1 1 0]\n",
            "True:[0 0 1 0 1 1 1 0]\n",
            "26 + 20 = 46\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.7661245445472114\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "100 + 84 = 120\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.46551099801145046\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 0 0 0 1 0 0]\n",
            "58 + 10 = 100\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.3889055799147864\n",
            "Pred:[0 0 1 1 0 0 0 1]\n",
            "True:[0 0 1 1 0 0 0 1]\n",
            "23 + 26 = 49\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.41561753054349365\n",
            "Pred:[0 0 1 0 1 1 0 1]\n",
            "True:[0 0 1 0 1 0 0 1]\n",
            "40 + 1 = 45\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.6574519201828146\n",
            "Pred:[1 1 0 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "84 + 52 = 200\n",
            "------------\n",
            "iters:3400\n",
            "Loss:1.1990937615379422\n",
            "Pred:[0 0 1 0 0 1 0 0]\n",
            "True:[1 1 0 0 0 1 0 0]\n",
            "104 + 92 = 36\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.10895367410203768\n",
            "Pred:[0 0 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 1 1 1 1]\n",
            "48 + 15 = 63\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.47838373127415323\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "88 + 48 = 136\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.7121374869939506\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 0 1 1 0 0 1 1]\n",
            "55 + 124 = 131\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.019936545554645094\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "113 + 10 = 123\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.3944938494304341\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "15 + 89 = 96\n",
            "------------\n",
            "iters:4000\n",
            "Loss:1.277605205274785\n",
            "Pred:[0 1 0 0 1 1 1 1]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "114 + 61 = 79\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.03650642594519632\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "51 + 73 = 124\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.23977477415149226\n",
            "Pred:[0 1 0 0 0 1 1 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "10 + 60 = 70\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.6823287169396366\n",
            "Pred:[1 0 0 1 0 0 1 1]\n",
            "True:[1 1 0 1 1 0 1 1]\n",
            "92 + 127 = 147\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.1890182425462016\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "22 + 112 = 134\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.24178120771800996\n",
            "Pred:[1 1 0 0 1 0 1 0]\n",
            "True:[1 1 0 0 1 0 1 0]\n",
            "86 + 116 = 202\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.19289425481451503\n",
            "Pred:[0 1 0 0 0 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "38 + 61 = 67\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.0932573730895789\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "121 + 39 = 160\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.005049130385017294\n",
            "Pred:[0 1 1 1 0 0 1 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "73 + 42 = 115\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.01604734267738098\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "26 + 97 = 123\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.007863568623259324\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "25 + 29 = 54\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.14067840859733144\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "58 + 90 = 148\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.1267216439194708\n",
            "Pred:[0 1 1 1 1 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "96 + 26 = 122\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.3316402017894178\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "83 + 23 = 104\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.05324754936412193\n",
            "Pred:[1 1 1 1 0 1 0 1]\n",
            "True:[1 1 1 1 0 1 0 1]\n",
            "126 + 119 = 245\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.004258152524466925\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "31 + 88 = 119\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.12997795470793888\n",
            "Pred:[1 1 0 0 1 0 1 0]\n",
            "True:[1 1 0 0 1 0 1 0]\n",
            "114 + 88 = 202\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.00829911845137804\n",
            "Pred:[0 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "70 + 25 = 95\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.0009351788354275332\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "101 + 42 = 143\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.0006676069005666822\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "88 + 45 = 133\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.001628558315672211\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "13 + 85 = 98\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.3300233624884241\n",
            "Pred:[0 1 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 1 0]\n",
            "35 + 31 = 64\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.0007616204842544922\n",
            "Pred:[0 1 0 0 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "13 + 66 = 79\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.0707994472925408\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "55 + 21 = 76\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.013502955872681488\n",
            "Pred:[1 0 0 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "113 + 19 = 132\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.2900968425289468\n",
            "Pred:[1 1 0 1 1 0 0 0]\n",
            "True:[1 1 0 1 1 0 1 0]\n",
            "103 + 115 = 216\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.008345398848958914\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "54 + 49 = 103\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.03557685084142754\n",
            "Pred:[0 0 1 0 0 0 0 0]\n",
            "True:[0 0 1 0 0 0 0 0]\n",
            "25 + 7 = 32\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.25189927082619507\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "48 + 60 = 108\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.0006060849299057462\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "87 + 44 = 131\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.00019201274529481925\n",
            "Pred:[0 1 0 0 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "75 + 4 = 79\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.016928944652624883\n",
            "Pred:[1 0 1 0 1 1 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "61 + 111 = 172\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.0434895467160929\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "111 + 13 = 124\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.13901071049644678\n",
            "Pred:[0 0 0 0 1 0 1 0]\n",
            "True:[0 0 0 0 1 0 1 0]\n",
            "6 + 4 = 10\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.0001566141377979249\n",
            "Pred:[0 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "43 + 52 = 95\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.00030925080634441735\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "1 + 126 = 127\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.13333576141863898\n",
            "Pred:[0 1 1 1 0 1 1 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "108 + 10 = 118\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.0006508973455281265\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "99 + 60 = 159\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.013065282292782691\n",
            "Pred:[0 1 0 0 1 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "49 + 23 = 72\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.0036457191714964836\n",
            "Pred:[0 0 1 0 0 1 1 1]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "24 + 15 = 39\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.12591128162531787\n",
            "Pred:[0 1 0 0 1 0 1 0]\n",
            "True:[0 1 0 0 1 0 1 0]\n",
            "42 + 32 = 74\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.006467774309477535\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "102 + 17 = 119\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.013563383717310631\n",
            "Pred:[0 0 1 1 0 1 1 1]\n",
            "True:[0 0 1 1 0 1 1 1]\n",
            "30 + 25 = 55\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.00022124819365246072\n",
            "Pred:[0 0 0 1 1 0 1 1]\n",
            "True:[0 0 0 1 1 0 1 1]\n",
            "3 + 24 = 27\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.0009302874981567701\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "116 + 3 = 119\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.0010498141053462407\n",
            "Pred:[0 0 1 1 1 0 0 1]\n",
            "True:[0 0 1 1 1 0 0 1]\n",
            "8 + 49 = 57\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.0002018642167217887\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "69 + 13 = 82\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.01799076312042776\n",
            "Pred:[0 0 0 1 1 1 1 1]\n",
            "True:[0 0 0 1 1 1 1 1]\n",
            "14 + 17 = 31\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.12523854063090883\n",
            "Pred:[0 1 1 0 0 1 1 0]\n",
            "True:[0 1 1 0 0 1 1 0]\n",
            "42 + 60 = 102\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.0007416177007385005\n",
            "Pred:[1 0 0 1 1 1 0 1]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "73 + 84 = 157\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.0021150094392182925\n",
            "Pred:[1 1 0 0 1 1 0 1]\n",
            "True:[1 1 0 0 1 1 0 1]\n",
            "78 + 127 = 205\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.9516003407665126\n",
            "Pred:[0 1 0 0 0 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "84 + 80 = 68\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.880955947688385\n",
            "Pred:[0 1 0 1 1 0 0 1]\n",
            "True:[1 1 1 0 0 0 0 1]\n",
            "111 + 114 = 89\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.6278218669612996\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[1 1 1 0 0 0 1 0]\n",
            "100 + 126 = 98\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.770018399695188\n",
            "Pred:[1 0 0 1 1 1 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "43 + 66 = 157\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.5685566329466658\n",
            "Pred:[0 0 0 0 0 1 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "82 + 18 = 4\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.824742321675992\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "46 + 19 = 1\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.7310317152356187\n",
            "Pred:[0 0 0 0 1 0 0 1]\n",
            "True:[1 0 1 0 1 0 0 1]\n",
            "57 + 112 = 9\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.5516472646703131\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "76 + 104 = 148\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.7181734522759042\n",
            "Pred:[1 1 1 0 1 0 1 0]\n",
            "True:[0 0 0 1 1 0 1 0]\n",
            "1 + 25 = 234\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGq1U45x28jR"
      },
      "source": [
        "ReLU関数では勾配爆発が、tanhでは9000回ごろから誤差が拡大することが分かる"
      ]
    }
  ]
}